{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import sys\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 124.08236622810364\n"
     ]
    }
   ],
   "source": [
    "# https://www.walkscore.com/cities-and-neighborhoods/states/\n",
    "# get html pages with tables \n",
    "\n",
    "res = requests.get(\"https://www.walkscore.com/cities-and-neighborhoods/states/\")\n",
    "soup = BeautifulSoup(res.text, features=\"html.parser\")\n",
    "\n",
    "state_table = soup.find(class_=\"state-list\")\n",
    "\n",
    "links = []\n",
    "for link in state_table.find_all(\"a\"):\n",
    "    links.append(link.get('href'))\n",
    "\n",
    "def get_html(links):\n",
    "    for l in links: \n",
    "        url = \"https://www.walkscore.com\"+l\n",
    "        t0 = time.time()\n",
    "        texts = requests.get(url)\n",
    "        text_utf = texts.text.encode(\"utf=8\")\n",
    "        if not os.path.exists(f'data/data_html'):\n",
    "            os.makedirs(f'data/data_html')\n",
    "\n",
    "        with open(f'data/data_html{l}.html', \"wb\") as output:\n",
    "            output.write(text_utf)\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "\n",
    "        response_delay = time.time() - t0\n",
    "        # wait 10x longer than it took them to respond\n",
    "        time.sleep(10*response_delay)\n",
    "            \n",
    "\n",
    "# check the time taken to retrieve data\n",
    "start_time = time.time()\n",
    "get_html(links)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken {end_time-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function process scraped html pages into text data and does a litle of feature engineering \n",
    "def met_data(state):\n",
    "    file_html = open(f\"data/data_html/{state}.html\", \"rb\")\n",
    "    plain_text = file_html.read()\n",
    "\n",
    "    tempD = []\n",
    "    finalD = []\n",
    "\n",
    "    soup = BeautifulSoup(plain_text, \"lxml\")\n",
    "    for tr in soup.findAll(\"td\", class_=lambda x: x != \"zipcode\"):\n",
    "    #     print(tr.get_text())\n",
    "        text = tr.get_text()\n",
    "        clean = re.sub(r'(\\n+){1,}', \"\", text)\n",
    "        tempD.append(clean)\n",
    "    # we have 5 columns (excluding zipcode)\n",
    "    rows = len(tempD) / 5\n",
    "\n",
    "    for times in range(round(rows)):\n",
    "        newTempD = [state]\n",
    "        for i in range(5):\n",
    "            newTempD.append(tempD[0])\n",
    "            tempD.pop(0)\n",
    "        finalD.append(newTempD)\n",
    "\n",
    "    return finalD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = met_data(\"AK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AK', 'Anchorage', '28', '21', '52', '291,826']\n",
      "['AK', 'Fairbanks', '33', '24', '57', '31,535']\n",
      "['AK', 'Juneau', '22', '18', '35', '31,275']\n",
      "['AK', 'Badger', '2', '--', '35', '19,482']\n"
     ]
    }
   ],
   "source": [
    "for row in f:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new folder to store cleaned data\n",
    "states = [\"AK\", \"AL\", \"AR\", \"AZ\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \"HI\", \"IA\", \"ID\", \"IL\", \"IN\", \"KS\", \n",
    "          \"KY\", \"LA\", \"MA\", \"MD\", \"ME\", \"MI\", \"MN\", \"MO\", \"MS\", \"MT\", \"NC\", \"ND\", \"NE\", \"NH\", \"NJ\", \"NM\", \"NV\", \n",
    "          \"NY\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\", \"WI\", \"WV\", \"WY\"]\n",
    "\n",
    "if not os.path.exists(\"data/cleaned_data\"):\n",
    "    os.makedirs(\"data/cleaned_data\")\n",
    "for state in states:\n",
    "    temp = met_data(state)\n",
    "    with open('data/cleaned_data/cleaned_' + str(state) + '.csv', 'w') as csvfile:\n",
    "        wr = csv.writer(csvfile, dialect='excel')\n",
    "        wr.writerow(['State', 'City', 'Walk Score', 'Transit Score', 'Bike Score', 'Population'])\n",
    "        for row in temp:\n",
    "            wr.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all gathered data into one file\n",
    "all_files = sorted(glob(\"data/cleaned_data/cleaned_*.csv\"))\n",
    "real_data = pd.concat((pd.read_csv(file,  encoding='latin-1') for file in all_files), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data.to_csv(\"data/walkability.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Walk Score</th>\n",
       "      <th>Transit Score</th>\n",
       "      <th>Bike Score</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AK</td>\n",
       "      <td>Anchorage</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>52</td>\n",
       "      <td>291,826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>33</td>\n",
       "      <td>24</td>\n",
       "      <td>57</td>\n",
       "      <td>31,535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AK</td>\n",
       "      <td>Juneau</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>31,275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AK</td>\n",
       "      <td>Badger</td>\n",
       "      <td>2</td>\n",
       "      <td>--</td>\n",
       "      <td>35</td>\n",
       "      <td>19,482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL</td>\n",
       "      <td>Birmingham (the largest city in Alabama)</td>\n",
       "      <td>35</td>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>212,237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State                                      City  Walk Score Transit Score  \\\n",
       "0    AK                                 Anchorage          28            21   \n",
       "1    AK                                 Fairbanks          33            24   \n",
       "2    AK                                    Juneau          22            18   \n",
       "3    AK                                    Badger           2            --   \n",
       "4    AL  Birmingham (the largest city in Alabama)          35            25   \n",
       "\n",
       "   Bike Score Population  \n",
       "0          52    291,826  \n",
       "1          57     31,535  \n",
       "2          35     31,275  \n",
       "3          35     19,482  \n",
       "4          31    212,237  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/walkability.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 6)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State            0\n",
       "City             0\n",
       "Walk Score       0\n",
       "Transit Score    0\n",
       "Bike Score       0\n",
       "Population       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
